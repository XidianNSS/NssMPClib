{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # Tutorial 5: Neural Network Inference\n",
    " Our library can support inference of neural networks based on secret sharing. Here we present a tutorial of neural network inference using secure two-party computation. Similar to Tutorial_2, we simulate multiple parties using multi-threads and trusted third parties which provide auxiliary parameters using local files. Models are shared before the prediction, and data is shared during the prediction process. You can refer to `./debug/application/neural_network/2pc/neural_network_server.py` and `./debug/application/neural_network/2pc/neural_network_client.py` for examples of actual usage of the neural network.\n",
    " In this tutorial, we use AlexNet as an example. First, train the model using `data/neural_network/AlexNet/Alexnet_MNIST_train.py`. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T10:02:15.236876Z",
     "start_time": "2024-04-12T10:02:14.573108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# training AlexNet\n",
    "exec(open('../data/neural_network/AlexNet/Alexnet_MNIST_train.py').read())"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And then, import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from data.neural_network.AlexNet.Alexnet import AlexNet\n",
    "from application.neural_network.model.model_converter import load_secure_model_from_file\n",
    "from config.base_configs import *\n",
    "from application.neural_network.party.neural_network_party import NeuralNetworkCS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T10:02:16.818196Z",
     "start_time": "2024-04-12T10:02:15.238489Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the server as the model provider and the client as the data provider, we need to generate triples for matrix multiplication in advance and distribute them to both parties. Similar to Tutorial_2, we simulate this process on the server side.\n",
    "The model provider also needs to import the following packages to share the model and data owner needs to import the following packages to share the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from application.neural_network.model.model_converter import share_and_save_model\n",
    "from application.neural_network.model.model_converter import share_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T10:02:16.834197Z",
     "start_time": "2024-04-12T10:02:16.819198Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can create our two parties."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import threading\n",
    "from config.network_configs import *\n",
    "\n",
    "\n",
    "server_server_address = (SERVER_IP, SERVER_SERVER_PORT)\n",
    "server_client_address = (SERVER_IP, SERVER_CLIENT_PORT)\n",
    "\n",
    "client_server_address = (CLIENT_IP, CLIENT_SERVER_PORT)\n",
    "client_client_address = (CLIENT_IP, CLIENT_CLIENT_PORT)\n",
    "\n",
    "# set Server\n",
    "server = NeuralNetworkCS(type='server')\n",
    "\n",
    "def set_server():\n",
    "    # CS connect\n",
    "    server.connect(server_server_address, server_client_address, client_server_address, client_client_address)\n",
    "\n",
    "# set Client\n",
    "client = NeuralNetworkCS(type='client')\n",
    "\n",
    "def set_client():\n",
    "    # CS connect\n",
    "    client.connect(client_server_address, client_client_address, server_server_address, server_client_address)\n",
    "\n",
    "server_thread = threading.Thread(target=set_server)\n",
    "client_thread = threading.Thread(target=set_client)\n",
    "\n",
    "server_thread.start()\n",
    "client_thread.start()\n",
    "client_thread.join()\n",
    "server_thread.join()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T10:02:17.961811Z",
     "start_time": "2024-04-12T10:02:16.836197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCPServer waiting for connection ......\n",
      "successfully connect to server 127.0.0.1:30000\n",
      "TCPServer waiting for connection ......\n",
      "successfully connect to server 127.0.0.1:20000\n",
      "TCPServer successfully connected by :('127.0.0.1', 20001)\n",
      "TCPServer successfully connected by :('127.0.0.1', 30001)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model provider needs to provide and share the model. We can choose whether to save param locally or not. For an example of another choice, see the C/S example in `debug/application/neural_network/2pc`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "net = AlexNet()\n",
    "net.load_state_dict(torch.load('./data/neural_network/AlexNet/AlexNet_MNIST.pkl'))\n",
    "\n",
    "share_and_save_model(model=net, save_path=model_file_path)  # share model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T10:02:18.228758Z",
     "start_time": "2024-04-12T10:02:17.963811Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data provider needs to provide data. Take an image in the MNIST dataset as an example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data = \"./data/img/image.png\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T10:02:18.244792Z",
     "start_time": "2024-04-12T10:02:18.232274Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because neural network inference involves matrix multiplication, before starting the prediction, we need to simulate one prediction and generate the required matrix Beaver triples in advance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def server_dummy_model():\n",
    "    server.dummy_model(net)\n",
    "\n",
    "def client_dummy_model():\n",
    "    client.dummy_model(data)\n",
    "\n",
    "server_thread = threading.Thread(target=server_dummy_model)\n",
    "client_thread = threading.Thread(target=client_dummy_model)\n",
    "\n",
    "server_thread.start()\n",
    "client_thread.start()\n",
    "client_thread.join()\n",
    "server_thread.join()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T10:02:19.374119Z",
     "start_time": "2024-04-12T10:02:18.246794Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above steps are the preparation work. Before starting inference, the data provider needs to share its data. And then, the two parties load their respective model shares and start inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def server_predict():\n",
    "    data_share = server.receive() # receive data share\n",
    "    net = load_secure_model_from_file(net=AlexNet(), path=model_file_path, party=server)\n",
    "    server.inference(net, data_share)\n",
    "\n",
    "    # close party after inference\n",
    "    server.close()\n",
    "\n",
    "\n",
    "def client_predict():\n",
    "    data_shares = share_data(data) # share data\n",
    "    data_share = data_shares[1]\n",
    "    client.send(data_shares[0]) # send shares to other party\n",
    "    net = load_secure_model_from_file(net=AlexNet(), path=model_file_path, party=client)\n",
    "    res = client.inference(net, data_share)\n",
    "\n",
    "    _, predicted = torch.max(res, 1)\n",
    "    # predicted_result\n",
    "    print('predicted result: ', predicted)\n",
    "\n",
    "    # close party after inference\n",
    "    client.close()\n",
    "\n",
    "server_thread = threading.Thread(target=server_predict)\n",
    "client_thread = threading.Thread(target=client_predict)\n",
    "\n",
    "server_thread.start()\n",
    "client_thread.start()\n",
    "client_thread.join()\n",
    "server_thread.join()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T10:02:27.976187Z",
     "start_time": "2024-04-12T10:02:19.376114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted result:  tensor([0], device='cuda:0')\n",
      "Communication costs:\n",
      "\tsend rounds: 88\t\tsend bytes: 3.078125 KB.\n",
      "\trecv rounds: 112\t\trecv bytes: 1.1753768920898438 MB.\n",
      "Communication costs:\n",
      "\tsend rounds: 112\t\tsend bytes: 1.1753768920898438 MB.\n",
      "\trecv rounds: 88\t\trecv bytes: 3.078125 KB.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see the prediction results as above, the core statements used by our library for neural network prediction are `server.inference` and `client.inference`. If you wish to perform additional operations on the prediction results, you can process them according to your specific requirements.\n",
    "In [data/neural_network/AlexNet/](https://github.com/XidianNSS/NssMPClib/tree/main/data/neural_network/AlexNet) and [data/neural_network/ResNet/](https://github.com/XidianNSS/NssMPClib/tree/main/data/neural_network/ResNet), we provide the training code for AlexNet and ResNet50. You can use them to train models according to your specific requirements and perform inference using trained models."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
